{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Flatiron's Intro to Web Scraping! \n",
    "\n",
    "![](https://media.giphy.com/media/1dcWGdOBg0wcU/giphy.gif)\n",
    "\n",
    "\n",
    "## A few things:\n",
    "- This file is called a **[Jupyter Notebook](https://jupyter.org/about)**. An industry standard tool for Data Science.\n",
    "\n",
    "- Each section of a jupyter notebook is called a `cell`. \n",
    "    - To run the code inside a cell, click into the cell and press `shift` + `enter`.\n",
    "\n",
    "    \n",
    "### Workshop Goals:\n",
    "1. Retrieve the HTML of a webpage with the `requests` library.\n",
    "2. Introduction to the `tree` structure of `HTML`.\n",
    "3. Use the `inspect` tool to sift through the HTML.\n",
    "4. Parse HTML with the `BeautifulSoup` library.\n",
    "5. Store data in a `csv` file using the `Pandas` library.\n",
    "\n",
    "# Let's scrape some data!\n",
    "The data we are scraping today will be from the [Quotes to Scrape](http://quotes.toscrape.com/) website.\n",
    "\n",
    "\n",
    "## Step 1:\n",
    "> **Import the necessary tools for our project**\n",
    "\n",
    "![](https://media.giphy.com/media/KcE7Dq5f8TTXzZ1LAF/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:45.620752Z",
     "start_time": "2020-07-03T20:17:45.025070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data organization\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "> **We use the `requests` library to connect to the website we wish to scrape.**\n",
    "\n",
    "<img src='https://media.giphy.com/media/eCwAEs05phtK/giphy.gif' width='300'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.001301Z",
     "start_time": "2020-07-03T20:17:45.622081Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ**A `Response 200` means our request was sucessful!** ‚úÖ\n",
    "\n",
    "‚ùåLet's take a quick look at an *unsuccessful* response. ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.257709Z",
     "start_time": "2020-07-03T20:17:46.003578Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_url = 'http://quotes.toscrape.com/20'\n",
    "requests.get(bad_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A `Response 404` means that the url you are using in your request is not pointing to a valid webpage.**\n",
    "\n",
    "<img src='https://media.giphy.com/media/VwoJkTfZAUBSU/giphy.gif' width='300'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "> **We collect the html from the website by adding `.text` to the end of the response variable.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.261804Z",
     "start_time": "2020-07-03T20:17:46.259332Z"
    }
   },
   "outputs": [],
   "source": [
    "html = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "> **We use `BeautifulSoup` to turn the html into something we can manipulate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.294964Z",
     "start_time": "2020-07-03T20:17:46.263257Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b><i><u>Very Soupy</u></i></b></h1></center>\n",
    "\n",
    "The name ***Beautiful Soup*** is an appropriate description. HTML does not make for a lovely reading experience. If you feel like you're staring at complete gibberish, you're not entirely wrong! HTML is a language designed for computers, not for human eyes.\n",
    "\n",
    "<img src='https://media.giphy.com/media/5xtDarBbqdSQxfGFdNS/giphy.gif' width=\"200\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fortunately for us,** <u>we do not have to read through every line of the html in order to web scrape.</u> \n",
    "\n",
    "Modern day web browsers come equipped with tools that allow us to easily sift through this soupy text.\n",
    "\n",
    "\n",
    "## Step 4\n",
    ">**We open up the page we're trying to scrape in a new tab.** <b><a href='http://quotes.toscrape.com/' target='_blank'>Click Here!</a></b> üëÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "   > **We create a list of every `div` that has a `class` of \"quote\"**\n",
    "\n",
    "**In this instance,** every item we want to collect is divided into identically labeled containers.\n",
    "- A div with a class of 'quote'.\n",
    "\n",
    "Not all HTML is as well organized as this page, but HTML is basically just a bunch of different organizational containers that we use to divide up text and other forms of media. Figuring out the organizational structure of a website is the entire process for web scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.299887Z",
     "start_time": "2020-07-03T20:17:46.296345Z"
    }
   },
   "outputs": [],
   "source": [
    "quote_divs = soup.find_all('div', {'class': 'quote'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "> **To figure out how to grab all the datapoints from a quote, we isolate a single quote, and work through the code for a single `div`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.303757Z",
     "start_time": "2020-07-03T20:17:46.301376Z"
    }
   },
   "outputs": [],
   "source": [
    "first_quote = quote_divs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we grab the text of the quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.309344Z",
     "start_time": "2020-07-03T20:17:46.306442Z"
    }
   },
   "outputs": [],
   "source": [
    "text = first_quote.find('span', {'class':'text'})\n",
    "text = text.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we grab the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.313824Z",
     "start_time": "2020-07-03T20:17:46.311221Z"
    }
   },
   "outputs": [],
   "source": [
    "author = first_quote.find('small', {'class': 'author'})\n",
    "author_name = author.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also grab the link that points to the author's bio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.317738Z",
     "start_time": "2020-07-03T20:17:46.315226Z"
    }
   },
   "outputs": [],
   "source": [
    "author_link = author.findNextSibling().attrs.get('href')\n",
    "author_link = url + author_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, let's grab all of the tags for the quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.322212Z",
     "start_time": "2020-07-03T20:17:46.318994Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_container = first_quote.find('div', {'class': 'tags'})\n",
    "tag_links = tag_container.find_all('a')\n",
    "\n",
    "tags = []\n",
    "for tag in tag_links:\n",
    "    tags.append(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.327644Z",
     "start_time": "2020-07-03T20:17:46.323644Z"
    }
   },
   "outputs": [],
   "source": [
    "print('text:', text)\n",
    "print('author name: ', author_name)\n",
    "print('author link: ', author_link)\n",
    "print('tags: ', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "> We create a function to make out code reusable.\n",
    "\n",
    "Now that we know how to collect this data from a quote div, we can compile the code into a [function](https://www.geeksforgeeks.org/functions-in-python/) called `quote_data`. This allows us to grab a quote div, feed it into the function like so...\n",
    "> `quote_data(quote_div)`\n",
    "\n",
    "...and receive all of the data from that div."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.334814Z",
     "start_time": "2020-07-03T20:17:46.329108Z"
    }
   },
   "outputs": [],
   "source": [
    "def quote_data(quote_div):\n",
    "    url = 'http://quotes.toscrape.com'\n",
    "    text = quote_div.find('span', {'class':'text'})\n",
    "    text = text.text\n",
    "\n",
    "    author = quote_div.find('small', {'class': 'author'})\n",
    "    author_name = author.text\n",
    "\n",
    "    author_link = author.findNextSibling().attrs.get('href')\n",
    "    author_link = url + author_link\n",
    "\n",
    "    tag_container = quote_div.find('div', {'class': 'tags'})\n",
    "\n",
    "    tag_links = tag_container.find_all('a')\n",
    "\n",
    "    tags = []\n",
    "    for tag in tag_links:\n",
    "        tags.append(tag.text)\n",
    "        \n",
    "    return {'author': author_name,\n",
    "            'text': text,\n",
    "            'author_link': author_link,\n",
    "            'tags': tags}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test our fuction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.339968Z",
     "start_time": "2020-07-03T20:17:46.336232Z"
    }
   },
   "outputs": [],
   "source": [
    "quote_data(quote_divs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect the data from every quote on the first page with a simple [```for loop```](https://www.w3schools.com/python/python_for_loops.asp)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.348380Z",
     "start_time": "2020-07-03T20:17:46.341351Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_one_data = []\n",
    "for div in quote_divs:\n",
    "    data_from_div = quote_data(div)\n",
    "    page_one_data.append(data_from_div)\n",
    "    \n",
    "print(len(page_one_data), 'quotes scraped!')\n",
    "page_one_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T18:55:14.762708Z",
     "start_time": "2020-07-03T18:55:14.758667Z"
    }
   },
   "source": [
    "# We just scraped an entire webpage!\n",
    "\n",
    "![](https://media.giphy.com/media/KiXl0vfc9XIIM/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level Up: What if we wanted to scrape the quotes from *every* page?\n",
    "\n",
    "**Step 1:** The first thing we do is take the code from above that scraped the data for all of the quotes on page one, and move it into a function called `parse_page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.353298Z",
     "start_time": "2020-07-03T20:17:46.350035Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_page(quote_divs):\n",
    "    data = []\n",
    "    for div in quote_divs:\n",
    "        div_data = quote_data(div)\n",
    "        data.append(div_data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** We grab the code we used at the very beginning to collect the html and the list of divs from a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.634218Z",
     "start_time": "2020-07-03T20:17:46.354642Z"
    }
   },
   "outputs": [],
   "source": [
    "base_url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "quote_divs = soup.find_all('div', {'class': 'quote'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** We feed all of the `quote_divs` into our newly made `parse_page` function to grab all of the data from that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.639287Z",
     "start_time": "2020-07-03T20:17:46.635708Z"
    }
   },
   "outputs": [],
   "source": [
    "data = parse_page(quote_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** We check to see if there is a `Next Page` button at the bottom of the page.\n",
    "\n",
    "*This is requires multiple steps.*\n",
    "\n",
    "1. We grab the outer container called that has a class of `pager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.644326Z",
     "start_time": "2020-07-03T20:17:46.640813Z"
    }
   },
   "outputs": [],
   "source": [
    "pager = soup.find('ul', {'class': 'pager'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no pager element on the webpage, pager will be set to `None`.\n",
    "\n",
    "2. We use an if check to make sure a pager exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.648478Z",
     "start_time": "2020-07-03T20:17:46.645876Z"
    }
   },
   "outputs": [],
   "source": [
    "if pager:\n",
    "    next_page = pager.find('li', {'class': 'next'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We then check to see if a `Next Page` button exists on the page. \n",
    "\n",
    "    - Every page has a next button except the *last* page which only has a `Previous Page` button. Basically, we're checking to see if the `Next button` exists. It it does, we \"click\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.652556Z",
     "start_time": "2020-07-03T20:17:46.649924Z"
    }
   },
   "outputs": [],
   "source": [
    "if next_page:\n",
    "    next_page = next_page.findChild('a')\\\n",
    "                         .attrs\\\n",
    "                         .get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With most webscraping tools, \"clicking a button\" means collecting the link inside the button and making a new request.\n",
    "\n",
    "If a link is pointing to a page on the same website, the links are usually just the forward slashes that need to be added to the base website url. This is called a `relative` link.\n",
    "\n",
    "**Step 5:** Collect the relative link that points to the next page, and add it to our base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.657345Z",
     "start_time": "2020-07-03T20:17:46.653925Z"
    }
   },
   "outputs": [],
   "source": [
    "next_page = base_url + next_page\n",
    "next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** We repeat the exact same process for this new link!\n",
    "\n",
    "ie:\n",
    "1. Make request using a url that points to the next page.\n",
    "2. Scrape quote divs\n",
    "3. Collect data from every quote div on that page\n",
    "4. Find the `Next page` button.\n",
    "5. Collect the url from the button\n",
    "6. Repeat\n",
    "\n",
    "So how do we do this over and over again without repeating ourselves?\n",
    "\n",
    "The first step is compile all of these steps into a new function called `scrape_quotes`.\n",
    "\n",
    "The second step is, something called `recursion`. \n",
    "\n",
    "<center><h1><u>Recursion</u></h1></center>\n",
    "\n",
    "![](https://media.giphy.com/media/l1J9R1Q7LJGSZOxFe/giphy.gif)\n",
    "\n",
    "> **Recursion** is a bit of a mind bend, so don't feel bad if it is hard to wrap your brain around. It took me a while to be able to understand recursive functions!\n",
    "\n",
    "Essentially, recursion is where we use a function inside of itself.\n",
    "\n",
    "**In this instance,** our code will be telling the computer, if there is a `Next page` button, rerun all of the code again on the page the next button points us to and check to see if there is a `Next page` button on *that* page. If there is, keep repeating the process until a `Next page` button is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:46.665745Z",
     "start_time": "2020-07-03T20:17:46.660460Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_quotes(url):\n",
    "    base_url = 'http://quotes.toscrape.com'\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    quote_divs = soup.find_all('div', {'class': 'quote'})\n",
    "    data = parse_page(quote_divs)\n",
    "    \n",
    "    pager = soup.find('ul', {'class': 'pager'})\n",
    "    if pager:\n",
    "        next_page = pager.find('li', {'class': 'next'})\n",
    "        \n",
    "        if next_page:\n",
    "            next_page =  next_page.findChild('a')\\\n",
    "                                  .attrs\\\n",
    "                                  .get('href')\n",
    "            \n",
    "            next_page = base_url + next_page\n",
    "            print('Scraping', next_page)\n",
    "            ## This is where the recursion happens\n",
    "            data += scrape_quotes(next_page)\n",
    "        \n",
    "    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set a variable called `data` that is the output of our recursive function!\n",
    "\n",
    "> A print statement has been added to output what page is being scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:49.433057Z",
     "start_time": "2020-07-03T20:17:46.667704Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = scrape_quotes(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize and explore our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:49.669754Z",
     "start_time": "2020-07-03T20:17:49.434678Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_tags(quote):\n",
    "    return len(quote['tags'])\n",
    "\n",
    "def tag_lengths(data):\n",
    "    lengths = []\n",
    "    for quote in data:\n",
    "        lengths.append(count_tags(quote))\n",
    "        \n",
    "    return lengths\n",
    "        \n",
    "lengths = tag_lengths(data)\n",
    "plt.figure(figsize=(10,6))   \n",
    "plt.hist(lengths, bins=9)\n",
    "plt.title('Number of Tags');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our data\n",
    "\n",
    "There are multiple ways to save data to a file. Pandas, `The Excel of Python` allows us to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:49.683276Z",
     "start_time": "2020-07-03T20:17:49.670975Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:17:49.689217Z",
     "start_time": "2020-07-03T20:17:49.684178Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('quotes_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping is a powerful tool. \n",
    "\n",
    "It can be used:\n",
    "- To discover the most in demand skills for thousands of online job postings.\n",
    "- To learn the average price of a product from thousands of online sale.\n",
    "- To research social media networks.\n",
    "\n",
    "**And so much more!** As our world continues to develop online markets and communities, the uses for webscraping continue to grow. In the end, the power of web scraping comes from the ability to create datasets that otherwise do not exist, or at the very least, are not readily available to the public.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
